# ğŸ›¡ï¸ CyberX Backend - Enterprise Cybersecurity Intelligence Platform

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-Latest-green.svg)](https://fastapi.tiangolo.com)
[![Docker](https://img.shields.io/badge/Docker-Ready-blue.svg)](https://docker.com)
[![Kubernetes](https://img.shields.io/badge/Kubernetes-Supported-326CE5.svg)](https://kubernetes.io)
[![Monitoring](https://img.shields.io/badge/Monitoring-Grafana%2BPrometheus-orange.svg)](https://grafana.com)
[![Auto-Scaling](https://img.shields.io/badge/Auto--Scaling-Enabled-success.svg)](#auto-scaling)

> **Enterprise-grade cybersecurity news aggregation and intelligence platform with real-time monitoring, auto-scaling, and comprehensive analytics.**

## ğŸš€ **What Makes This INCREDIBLE**

### **Enterprise Features**
- ğŸ”¥ **Real-time Log Monitoring** with live Grafana dashboards
- âš¡ **Auto-scaling** with Docker Compose, Swarm, and Kubernetes support
- ğŸ“Š **Complete Observability** with Prometheus metrics and alerting
- ğŸŒ **Load Balancing** with NGINX and health checks
- ğŸ›¡ï¸ **Production Ready** with security headers and rate limiting
- ğŸ”„ **Zero-downtime Deployments** with rolling updates

### **Cybersecurity Intelligence**
- ğŸ“° **Multi-source News Aggregation** from top cybersecurity feeds
- ğŸ¤– **AI-powered Summarization** for threat intelligence
- ğŸš¨ **Real-time Alerting** for critical security updates
- ğŸ“ˆ **Trend Analysis** and historical data tracking
- ğŸ” **Advanced Search** and filtering capabilities
- ğŸ“± **RESTful API** for integration with other tools

### **Developer Experience**
- âš¡ **One-command Deployment** for entire stack
- ğŸ”§ **Hot Reload** for development
- ğŸ“ **Comprehensive Documentation** with examples
- ğŸ§ª **Built-in Testing** and health checks
- ğŸ” **Live Debugging** through logs and metrics
- ğŸ”„ **Cross-platform** (Windows/Linux) compatibility

## ğŸ“‹ **Quick Start**

### **ğŸ”¥ Super Quick (2 Commands)**
```bash
# 1. Start monitoring stack
./start-stack-simple.ps1 -Monitoring

# 2. Start backend API
python -m uvicorn api.cybersecurity_fastapi:app --host 0.0.0.0 --port 8080 --reload
```

### **ğŸš€ Auto-Scaling Production**
```bash
# Deploy with auto-scaling (3 replicas + load balancer)
python deploy_scaling.py compose --test
```

### **â˜¸ï¸ Kubernetes Enterprise**
```bash
# Deploy to Kubernetes with HPA
python deploy_scaling.py kubernetes --test
```

## ğŸ“Š **Access Your Services**

| Service | URL | Purpose |
|---------|-----|---------|
| **ğŸ”¹ FastAPI Backend** | http://localhost:8080 | Main API & Documentation |
| **ğŸ“Š Grafana Dashboard** | http://localhost:3000 | Monitoring & Analytics |
| **ğŸ“ˆ Prometheus** | http://localhost:9091 | Metrics Collection |
| **ğŸ”§ Backend Metrics** | http://localhost:9200/metrics | Application Metrics |
| **ğŸ“ Live Log Metrics** | http://localhost:9201/metrics | Real-time Log Analysis |
| **ğŸš¨ Alertmanager** | http://localhost:9093 | Alert Management |

**Default Credentials:**
- Grafana: `admin` / `cybersec2024`

For detailed setup instructions, see [STARTUP.md](STARTUP.md)

---

**â­ If you find this project useful, please give it a star! â­**

Built with â¤ï¸ for the cybersecurity community ğŸ›¡ï¸

## ğŸš€ System Overview

This backend system provides:
- **ğŸŒ Multi-Source News Scraping**: 7 major cybersecurity websites
- **ğŸ¤– AI-Powered Summarization**: Google Gemini integration for intelligent content processing
- **âš¡ Production-Ready FastAPI**: High-performance REST API with automatic documentation
- **ğŸ³ Docker Containerization**: Scalable deployment with health monitoring
- **ğŸ“Š Automated Data Management**: Smart backup rotation and file organization
- **ï¿½ Real-Time Processing**: Live feed updates and continuous monitoring
- **ï¿½ï¸ Enterprise-Grade**: Built for reliability, scalability, and maintainability

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CYBERSECURITY BACKEND                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸŒ WEB SCRAPING LAYER                                         â”‚
â”‚  â”œâ”€â”€ Enhanced Crawl4AI Engine                                  â”‚
â”‚  â”œâ”€â”€ Multi-Site Coordination                                   â”‚
â”‚  â”œâ”€â”€ Anti-Bot Protection                                       â”‚
â”‚  â””â”€â”€ URL Deduplication                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ¤– AI PROCESSING LAYER                                        â”‚
â”‚  â”œâ”€â”€ Google Gemini Integration                                 â”‚
â”‚  â”œâ”€â”€ Intelligent Summarization                                 â”‚
â”‚  â”œâ”€â”€ Content Enhancement                                       â”‚
â”‚  â””â”€â”€ Progressive Processing                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“Š DATA MANAGEMENT LAYER                                      â”‚
â”‚  â”œâ”€â”€ Automated Backup System                                   â”‚
â”‚  â”œâ”€â”€ Date-Based File Organization                              â”‚
â”‚  â”œâ”€â”€ URL Tracking & Deduplication                              â”‚
â”‚  â””â”€â”€ Smart Data Rotation                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸŒ API LAYER (FastAPI)                                        â”‚
â”‚  â”œâ”€â”€ RESTful Endpoints                                         â”‚
â”‚  â”œâ”€â”€ Interactive Documentation                                 â”‚
â”‚  â”œâ”€â”€ Health Monitoring                                         â”‚
â”‚  â””â”€â”€ CORS & Security                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ³ DEPLOYMENT LAYER                                           â”‚
â”‚  â”œâ”€â”€ Docker Containerization                                   â”‚
â”‚  â”œâ”€â”€ Volume Persistence                                        â”‚
â”‚  â”œâ”€â”€ Health Checks                                             â”‚
â”‚  â””â”€â”€ Auto-Restart Policies                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ï¿½ Project Structure

```
backend/
â”œâ”€â”€ ğŸš€ MAIN LAUNCHERS
â”‚   â”œâ”€â”€ main_launch.py              # Primary system launcher with modes
â”‚   â”œâ”€â”€ auto_start.py               # One-command full automation
â”‚   â””â”€â”€ health_check.py             # System health validation
â”‚
â”œâ”€â”€ ğŸŒ API LAYER
â”‚   â””â”€â”€ api/
â”‚       â””â”€â”€ cybersecurity_fastapi.py # FastAPI server with full endpoints
â”‚
â”œâ”€â”€ ğŸ¤– AI PROCESSING
â”‚   â””â”€â”€ Ai/
â”‚       â””â”€â”€ enhanced_ai_summarizer.py # Google Gemini AI integration
â”‚
â”œâ”€â”€ ğŸ”„ AUTOMATION SERVICES
â”‚   â””â”€â”€ automation/
â”‚       â””â”€â”€ automation_service.py    # Background automation & scheduling
â”‚
â”œâ”€â”€ ğŸ› ï¸ CORE MODULES
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ scrapers/
â”‚       â”‚   â””â”€â”€ enhanced_cybersec_crawler.py # Advanced web scraping
â”‚       â”œâ”€â”€ monitoring/
â”‚       â”‚   â””â”€â”€ [monitoring modules]         # Real-time monitoring
â”‚       â””â”€â”€ utils/
â”‚           â””â”€â”€ backup_manager.py            # Data backup & management
â”‚
â”œâ”€â”€ ğŸ³ DEPLOYMENT
â”‚   â””â”€â”€ docker/
â”‚       â”œâ”€â”€ docker-compose.yml       # Container orchestration
â”‚       â”œâ”€â”€ Dockerfile              # Container configuration
â”‚       â””â”€â”€ .dockerignore           # Build optimization
â”‚
â”œâ”€â”€ âš™ï¸ CONFIGURATION
â”‚   â””â”€â”€ config/
â”‚       â””â”€â”€ url_fetch.txt           # Target websites configuration
â”‚
â”œâ”€â”€ ğŸ“Š DATA STORAGE
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ News_today_YYYYMMDD.json      # Daily news archives
â”‚       â”œâ”€â”€ cybersecurity_news_live.json  # Live processing feed
â”‚       â”œâ”€â”€ summarized_news_hf.json       # AI-processed summaries
â”‚       â”œâ”€â”€ url_scraped.txt               # Scraped URL tracking
â”‚       â”œâ”€â”€ url_final_summarized.txt      # Summarized URL tracking
â”‚       â”œâ”€â”€ backup/                       # Automatic backups
â”‚       â””â”€â”€ alerts/                       # Alert system data
â”‚
â”œâ”€â”€ ğŸ“‹ LOGS & MONITORING
â”‚   â””â”€â”€ logs/
â”‚       â”œâ”€â”€ fastapi.log             # API server logs
â”‚       â”œâ”€â”€ backend.log             # System operation logs
â”‚       â””â”€â”€ heartbeat.json          # Health monitoring data
â”‚
â””â”€â”€ ï¿½ DOCUMENTATION
    â”œâ”€â”€ README.md                   # This comprehensive guide
    â”œâ”€â”€ requirements.txt            # Python dependencies
    â”œâ”€â”€ env.example                 # Environment variables template
    â””â”€â”€ FULL_AUTOMATION_GUIDE.md    # Complete automation setup
```

---
# ğŸš€ Quick Setup Guide

## Prerequisites
- Docker Desktop for Windows installed
- At least 2GB free RAM
- Ports 80 and 8080 available

## 1. Quick Deploy (Recommended)

### Option A: API Only (Recommended for testing)
```powershell
.\deploy.ps1 -Command deploy -Mode api-only
```

### Option B: Full Stack with Nginx Load Balancer
```powershell
.\deploy.ps1 -Command deploy
```

## 2. Manual Deploy

### Build and Run
```powershell
# Build image
docker build -t cybersecurity-fastapi:latest .

# Run single container (simple)
docker run -d --name cybersec-api -p 8080:8080 -v "${PWD}/data:/app/data" -v "${PWD}/logs:/app/logs" cybersecurity-fastapi:latest

# OR run with Docker Compose (advanced)
docker-compose up -d
```

## 3. Verify Deployment

- âœ… Health Check: http://localhost:8080/health
- ğŸ“Š API Docs: http://localhost:8080/docs
- ğŸ” Interactive API: http://localhost:8080/redoc
- ğŸš€ Your API: http://localhost:8080/api/news

## 4. Scale for Production

The setup includes:
- âœ… **Uvicorn** with multiple workers (4 workers)
- âœ… **Health checks** and auto-restart
- âœ… **Volume persistence** for data
- âœ… **Security headers** and CORS
- âœ… **Non-root user** execution
- âœ… **Optimized Docker image**

## 5. Monitor and Maintain

```powershell
# View logs
.\deploy.ps1 -Command logs

# Check status
.\deploy.ps1 -Command status

# Update application
.\deploy.ps1 -Command update

# Stop services
.\deploy.ps1 -Command stop
```

## 6. Test Your API

```powershell
# Health check
curl http://localhost:8080/health

# Get news
curl http://localhost:8080/api/news

# Get sources
curl http://localhost:8080/api/news/sources

# Search news
curl "http://localhost:8080/api/news/search?q=cybersecurity"
```

## ğŸ¯ You're Production Ready!

Your FastAPI app can now handle:
- **Concurrent requests** with 4 workers
- **Auto-scaling** with container orchestration
- **Health monitoring** and auto-restart
- **Persistent data** across restarts
- **Zero-downtime deployments**

## âš¡ Performance Tips

1. **For VPS deployment**: Use docker-compose with nginx
2. **For high traffic**: Scale workers in the Dockerfile
3. **For SSL**: Configure nginx with certificates
4. **For monitoring**: Add prometheus metrics

Deploy to your VPS server and enjoy scalable, high-performance API! ğŸš€


# ğŸš€ Full Automation Guide - Cybersecurity News Crawler v4.0

## One-Command Solution - No More Manual Work!

Your cybersecurity news crawler now supports **complete automation**! Just run one command and everything happens automatically.

## ğŸ¯ Quick Start - The Easy Way

### Option 1: Super Simple Auto-Start
```bash
python auto_start.py
```

### Option 2: Using Main Launcher
```bash
python main_launch.py --mode full-auto
```

### Option 3: Interactive Menu (choose option 0)
```bash
python main_launch.py
# Then select [0] FULL AUTO MODE
```

## ğŸ”„ What Happens Automatically

When you run the full auto mode, here's the complete sequence:

### Phase 1: ğŸ“¦ Automatic Backup (30 seconds)
- Scans for old data files from previous days
- Automatically moves them to `data/backup/` folder
- Keeps your main directory clean
- **No prompts** - happens automatically

### Phase 2: ğŸ”§ Setup & Validation (1-2 minutes)
- Checks Python version compatibility
- Validates all required files exist
- Installs/updates dependencies automatically
- Verifies project structure
- **No prompts** - auto-fixes what it can

### Phase 3: ğŸ“° Data Preparation (2-5 minutes)
- Checks if recent data exists
- If no data: Runs initial scraping automatically
- If data exists: Skips to next phase
- **No prompts** - intelligent decision making

### Phase 4: ğŸŒ API Server Startup (10 seconds)
- Starts FastAPI server in background
- Available at http://localhost:8080/
- Interactive docs at http://localhost:8080/docs
- Fallback to Flask if FastAPI fails
- **No prompts** - automatic server selection

### Phase 5: ğŸ”” Alert System Startup (5 seconds)
- Starts real-time notification system
- Monitors file changes
- Sends alerts to connected clients
- **No prompts** - runs in background

### Phase 6: ğŸ¤– Initial AI Processing (1-3 minutes)
- Runs AI summarization on existing data
- Creates enhanced summaries
- Prepares data for monitoring
- **No prompts** - automatic processing

### Phase 7: âš¡ 24/7 Monitoring (Continuous)
- Checks websites every 30 minutes
- Only processes NEW articles
- Automatically runs AI on new content
- Sends real-time alerts
- **No prompts** - fully autonomous

## ğŸ“Š System Status Dashboard

Once running, you'll see:

```
ğŸ‰ SYSTEM FULLY OPERATIONAL!
ğŸ“Š Current Status:
   ğŸŒ API Server: âœ… Running
   ğŸ”” Alert System: âœ… Running  
   ğŸ¤– AI Processing: âœ… Enabled
   ğŸ“ Data Files: âœ… Available

ğŸ”„ Starting continuous monitoring...
â° Monitor will check for new articles every 30 minutes
ğŸ¤– AI will process new articles automatically
ğŸ”” Alerts will be sent to connected clients
```

## ğŸ›‘ How to Stop

Simply press `Ctrl+C` and the system will:
1. Stop all background processes gracefully
2. Clean up resources
3. Show completion summary

## ğŸ“ˆ Performance & Resources

### System Requirements
- **CPU**: Low usage (< 5% average)
- **Memory**: ~200-500 MB total
- **Network**: Minimal (only during scraping cycles)
- **Storage**: Auto-managed with backup system

### Background Processes
When running, you'll have these processes:
1. **Main Monitor** (cycles every 30 min)
2. **FastAPI Server** (port 8080)
3. **Alert System** (file watcher)
4. **AI Processor** (on-demand)

## ğŸ”§ Customization Options

### Environment Variables
```bash
# Monitoring interval (default: 30 minutes)
set AUTONOMOUS_INTERVAL_MINUTES=15

# API server port (default: 8080)
set UVICORN_PORT=9000

# Maximum cycles for testing (optional)
set AUTONOMOUS_MAX_CYCLES=5
```

### Command Line Options
```bash
# Full auto with custom interval
python main_launch.py --mode full-auto

# Skip setup validation (faster restart)
python main_launch.py --mode full-auto --no-setup

# Individual components
python main_launch.py --mode api        # Just API server
python main_launch.py --mode monitor    # Just monitoring
python main_launch.py --mode ai         # Just AI processing
```

## ğŸ“± Accessing Your Data

### Web Interface
- **API Base**: http://localhost:8080/
- **Documentation**: http://localhost:8080/docs
- **Health Check**: http://localhost:8080/api/health

### Key Endpoints
```
GET  /api/news              # All latest news
GET  /api/news/sources      # Available sources  
GET  /api/news/search?q=    # Search articles
GET  /api/stats             # System statistics
GET  /api/alerts            # Recent alerts
POST /api/google-news/search # Google News search
```

### File Locations
```
data/
â”œâ”€â”€ summarized_news_hf.json     # AI-processed articles
â”œâ”€â”€ cybersecurity_news_live.json # Live monitoring data
â”œâ”€â”€ News_today_YYYYMMDD.json    # Daily archives
â””â”€â”€ backup/                     # Old files (auto-managed)

logs/
â”œâ”€â”€ fastapi.log                 # API server logs
â”œâ”€â”€ backend.log                 # General system logs
â””â”€â”€ heartbeat.json              # System health data
```

## ğŸ”’ Security & Reliability

### Automatic Recovery
- **Process Monitoring**: Restarts failed components
- **Error Handling**: Continues operation despite individual failures
- **Data Backup**: Automatic file management
- **Health Checks**: Built-in system monitoring

### Safe Operation
- **Graceful Shutdown**: Clean process termination
- **Resource Management**: Automatic cleanup
- **Error Logging**: Comprehensive logging for debugging
- **Data Integrity**: File validation and backup

## ğŸ†˜ Troubleshooting

### Common Issues

**"Dependencies failed to install"**
```bash
pip install --upgrade pip
pip install -r requirements.txt
```

**"API server failed to start"**
- Check if port 8080 is available
- Try different port: `set UVICORN_PORT=9000`
- Use Flask fallback (automatic)

**"No data files found"**
- Check internet connection
- Verify `config/url_fetch.txt` exists
- Run manual scrape first: `python main_launch.py --mode scrape`

**"AI processing failed"**
- Check Google API key in environment
- Verify `Ai/enhanced_ai_summarizer.py` exists
- System continues without AI if needed

### Manual Recovery
If something goes wrong, you can always:
```bash
# Reset and restart
python main_launch.py --mode setup    # Validate setup
python main_launch.py --mode status   # Check system
python main_launch.py --mode full-auto # Restart automation
```

## ğŸ¯ Best Practices

### For Daily Use
1. **Start once in the morning**: `python auto_start.py`
2. **Let it run all day**: Fully autonomous operation
3. **Check web interface**: http://localhost:8080/docs
4. **Stop at night**: `Ctrl+C` for clean shutdown

### For Development
1. **Use test mode**: `python main_launch.py --mode test`
2. **Check logs**: Monitor `logs/` directory
3. **Individual testing**: Run components separately
4. **Status monitoring**: `python main_launch.py --mode status`

### For Production
1. **Use environment variables**: Configure intervals and ports
2. **Monitor resources**: Check system performance
3. **Regular backups**: Built-in automatic backup system
4. **Health monitoring**: Use `/api/health` endpoint

## ğŸ‰ Benefits Summary

âœ… **Zero Manual Intervention**: Set it and forget it
âœ… **Complete Automation**: Everything happens automatically  
âœ… **Intelligent Processing**: Only processes new content
âœ… **Real-time Access**: Live API and web interface
âœ… **Automatic Backup**: Data management handled
âœ… **Error Recovery**: Continues despite individual failures
âœ… **Performance Optimized**: Low resource usage
âœ… **Easy Monitoring**: Web dashboard and logs

## ğŸš€ You're All Set!

Your cybersecurity news system is now fully automated. Just run:

```bash
python auto_start.py
```

And everything will work automatically! ğŸ‰

---

**Need Help?** Check the logs in the `logs/` directory or use the interactive mode to test individual 
components.

# ğŸ›¡ï¸ Cybersecurity FastAPI - Docker Deployment Guide

This guide will help you containerize and deploy your FastAPI application for production use with high availability and scalability.

## ğŸš€ Features

- **Production-ready containerization** with multi-stage builds
- **Horizontal scaling** with Gunicorn workers
- **Load balancing** with Nginx reverse proxy
- **Health checks** and monitoring
- **Rate limiting** and security headers
- **Automated deployment scripts**
- **Volume persistence** for data and logs
- **Graceful shutdowns** and restarts

## ğŸ“‹ Prerequisites

- Docker Desktop for Windows (or Docker Engine on Linux)
- Docker Compose
- At least 2GB RAM for optimal performance
- Open ports: 80 (HTTP), 8080 (API)

## ğŸ—ï¸ Architecture

```
Internet â†’ Nginx (Port 80) â†’ FastAPI (Port 8080) â†’ Your Backend Services
```

### Components:
- **Nginx**: Reverse proxy, load balancer, rate limiting, SSL termination
- **FastAPI**: Your API application running with Gunicorn
- **Volumes**: Persistent storage for data, logs, and configuration

## ğŸ› ï¸ Setup Instructions

### 1. Quick Start (Recommended)

**Windows PowerShell:**
```powershell
# Build and deploy with nginx
.\deploy.ps1 -Command deploy

# Or deploy API only (without nginx)
.\deploy.ps1 -Command deploy -Mode api-only
```

**Linux/macOS:**
```bash
# Make script executable
chmod +x deploy.sh

# Build and deploy with nginx
./deploy.sh deploy

# Or deploy API only (without nginx)
./deploy.sh deploy api-only
```

### 2. Manual Deployment

**Step 1: Build the Docker image**
```bash
docker build -t cybersecurity-fastapi:latest .
```

**Step 2: Deploy with Docker Compose (Full stack with nginx)**
```bash
docker-compose up -d
```

**Step 3: Deploy API only (Without nginx)**
```bash
docker run -d \
  --name cybersecurity-fastapi \
  -p 8080:8080 \
  -v "$(pwd)/data:/app/data" \
  -v "$(pwd)/logs:/app/logs" \
  -v "$(pwd)/config:/app/config" \
  --restart unless-stopped \
  cybersecurity-fastapi:latest
```

## ğŸ”§ Configuration

### Environment Variables

Create a `.env` file in your project root:

```env
# Production settings
ENVIRONMENT=production
API_HOST=0.0.0.0
API_PORT=8080
API_WORKERS=4

# Security
SECRET_KEY=your-secret-key-here
ALLOWED_HOSTS=*

# Performance
WORKER_TIMEOUT=120
MAX_REQUESTS=1000
RATE_LIMIT_PER_MINUTE=60

# Logging
LOG_LEVEL=INFO
```

### Scaling Configuration

**Gunicorn Workers**: Automatically set to `CPU_CORES * 2 + 1`
- For 4-core server: 9 workers
- For 8-core server: 17 workers

**Memory Usage**: Each worker uses ~100-200MB RAM
- 4 workers: ~800MB RAM
- 8 workers: ~1.6GB RAM

### Custom Domain and SSL

1. **Update nginx configuration**:
   ```bash
   # Edit nginx/nginx.conf
   server_name your-domain.com;
   ```

2. **Add SSL certificates**:
   ```bash
   # Place certificates in nginx/ssl/
   nginx/ssl/cert.pem
   nginx/ssl/key.pem
   ```

3. **Uncomment HTTPS server block** in `nginx/nginx.conf`

## ğŸ“Š Monitoring and Health Checks

### Health Check Endpoints

- **Simple Health**: `http://localhost:8080/health`
- **Detailed Health**: `http://localhost:8080/api/health`
- **API Documentation**: `http://localhost:8080/docs`

### Checking Application Status

```powershell
# PowerShell
.\deploy.ps1 -Command status

# Linux/macOS
./deploy.sh status
```

### View Logs

```powershell
# PowerShell - View all logs
.\deploy.ps1 -Command logs -Mode compose

# Linux/macOS - View all logs
./deploy.sh logs compose

# Docker Compose logs
docker-compose logs -f

# API container logs only
docker logs -f cybersecurity-fastapi
```

## ğŸ”„ Updates and Maintenance

### Update Application

```powershell
# PowerShell
.\deploy.ps1 -Command update

# Linux/macOS
./deploy.sh update
```

### Stop Services

```powershell
# PowerShell
.\deploy.ps1 -Command stop -Mode compose

# Linux/macOS
./deploy.sh stop compose
```

### Restart Services

```bash
docker-compose restart
```

## âš¡ Performance Optimization

### 1. Resource Limits

Add to `docker-compose.yml`:
```yaml
services:
  cybersecurity-api:
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G
```

### 2. Database Connection Pooling

If using a database, add connection pooling:
```python
# Add to your FastAPI app
from sqlalchemy import create_engine
from sqlalchemy.pool import QueuePool

engine = create_engine(
    DATABASE_URL,
    poolclass=QueuePool,
    pool_size=20,
    max_overflow=30,
    pool_pre_ping=True
)
```

### 3. Redis Caching

Add Redis for caching (optional):
```yaml
# Add to docker-compose.yml
redis:
  image: redis:alpine
  container_name: cybersecurity-redis
  restart: unless-stopped
  networks:
    - cybersec-network
```

## ğŸ›¡ï¸ Security Best Practices

### 1. Rate Limiting (Already configured)
- 10 requests per second per IP
- Burst capacity of 20 requests
- Connection limit of 20 per IP

### 2. Security Headers (Already configured)
- X-Frame-Options: DENY
- X-Content-Type-Options: nosniff
- X-XSS-Protection: 1; mode=block
- Strict-Transport-Security (for HTTPS)

### 3. Additional Security
```bash
# Run containers as non-root user (already configured)
# Use secrets for sensitive data
echo "your-secret-key" | docker secret create api_secret -

# Regularly update base images
docker pull python:3.11-slim
docker build --no-cache -t cybersecurity-fastapi:latest .
```

## ğŸ“ˆ Scaling for High Traffic

### Horizontal Scaling

**Option 1: Multiple containers**
```bash
docker-compose up --scale cybersecurity-api=3
```

**Option 2: Docker Swarm**
```bash
# Initialize swarm
docker swarm init

# Deploy stack
docker stack deploy -c docker-compose.yml cybersec-stack
```

**Option 3: Kubernetes**
Use the provided Kubernetes manifests (if created) for enterprise scaling.

### Load Testing

Test your deployment:
```bash
# Install artillery (load testing tool)
npm install -g artillery

# Create load test config
artillery quick --count 100 --num 10 http://localhost:8080/health
```

## ğŸ› Troubleshooting

### Common Issues

**1. Port already in use**
```bash
# Find process using port 8080
netstat -tulpn | grep 8080

# Kill process
sudo kill -9 <PID>
```

**2. Container won't start**
```bash
# Check container logs
docker logs cybersecurity-fastapi

# Check container status
docker ps -a
```

**3. Health check failing**
```bash
# Test health endpoint manually
curl -f http://localhost:8080/health

# Check if all required files exist
docker exec cybersecurity-fastapi ls -la /app/data/
```

**4. High memory usage**
```bash
# Monitor container resources
docker stats cybersecurity-fastapi

# Reduce worker count in gunicorn.conf.py
workers = 2  # Reduce from default
```

### Debug Mode

Enable debug mode for development:
```yaml
# In docker-compose.yml
environment:
  - ENVIRONMENT=development
  - DEBUG=true
```

## ğŸ“ Support

If you encounter issues:

1. Check the logs: `docker-compose logs -f`
2. Verify health endpoints are responding
3. Ensure all required volumes are mounted
4. Check firewall settings for ports 80 and 8080

## ğŸ¯ Production Deployment Checklist

- [ ] Environment variables configured
- [ ] SSL certificates installed (for HTTPS)
- [ ] Firewall rules configured
- [ ] Domain name configured
- [ ] Health checks working
- [ ] Backup strategy implemented
- [ ] Monitoring and alerting setup
- [ ] Load testing completed
- [ ] Security scan performed

Your FastAPI application is now production-ready and scalable! ğŸš€


**Built with â¤ï¸ for the cybersecurity community**

*Last updated: August 18, 2025*

## âš¡ Quick Start Guide

### ğŸ¯ One-Command Launch (Recommended)
```powershell
# Full system startup with all components
python auto_start.py

# OR using main launcher
python main_launch.py --mode full-auto
```

### ğŸ”§ Step-by-Step Setup

#### 1. Environment Setup
```powershell
# 1. Clone/Download the project
cd C:\your\path\backend

# 2. Create virtual environment (recommended)
python -m venv venv
.\venv\Scripts\Activate.ps1

# 3. Install dependencies
pip install -r requirements.txt

# 4. Validate setup
python main_launch.py --mode setup
```

#### 2. Manual Component Testing
```powershell
# Test scraping system
python main_launch.py --mode scrape

# Test AI processing
python Ai\enhanced_ai_summarizer.py

# Test API server
python main_launch.py --mode api

# Check system status
python main_launch.py --mode status
```

#### 3. Production Deployment
```powershell
# Docker deployment (production-ready)
docker-compose -f docker\docker-compose.yml up -d

# Verify deployment
curl http://localhost:8080/health
```

## ğŸŒ Supported News Sources

| Website | URL | Stealth Level | Avg Articles |
|---------|-----|---------------|--------------|
| **The Hacker News** | thehackernews.com | Low | 8-10 |
| **The420.in** | the420.in | Low | 8-10 |
| **Bleeping Computer** | bleepingcomputer.com | Maximum | 8-10 |
| **Krebs on Security** | krebsonsecurity.com | High | 7-8 |
| **Dark Reading** | darkreading.com | High | 0-8* |
| **Security Week** | securityweek.com | Medium | 8 |
| **Threat Post** | threatpost.com | Medium | 8 |

*Dark Reading occasionally blocks automated access

## ğŸ”„ System Workflow

### Daily Operation Cycle
```
1. ğŸŒ… MORNING STARTUP
   â”œâ”€â”€ Auto-backup previous day's files
   â”œâ”€â”€ Initialize clean data directory
   â””â”€â”€ Start all services

2. ğŸ”„ CONTINUOUS OPERATION
   â”œâ”€â”€ Scrape news every 30 minutes (default)
   â”œâ”€â”€ Process new articles with AI
   â”œâ”€â”€ Update API endpoints
   â””â”€â”€ Maintain live JSON feeds

3. ğŸŒ™ EVENING BACKUP
   â”œâ”€â”€ Archive daily files to backup/
   â”œâ”€â”€ Clean up temporary files
   â””â”€â”€ Prepare for next day
```

### Data Processing Pipeline
```
ğŸ“° NEWS SCRAPING
â”œâ”€â”€ Enhanced Crawl4AI Engine
â”œâ”€â”€ Site-specific configurations
â”œâ”€â”€ Anti-bot protection
â”œâ”€â”€ Content extraction
â””â”€â”€ URL deduplication

ğŸ¤– AI PROCESSING
â”œâ”€â”€ Google Gemini integration
â”œâ”€â”€ Intelligent summarization
â”œâ”€â”€ Content enhancement
â”œâ”€â”€ Progressive processing (20 articles/batch)
â””â”€â”€ Quality validation

ğŸ’¾ DATA MANAGEMENT
â”œâ”€â”€ Date-based file naming
â”œâ”€â”€ Automatic backup rotation
â”œâ”€â”€ URL tracking & deduplication
â”œâ”€â”€ Live feed management
â””â”€â”€ Archive organization
```

## ğŸ› ï¸ Configuration Management

### Environment Variables
Create `.env` file in project root:
```env
# Google AI Configuration
GOOGLE_API_KEY=your_gemini_api_key_here

# API Server Configuration
UVICORN_HOST=0.0.0.0
UVICORN_PORT=8080
UVICORN_WORKERS=4

# Automation Settings
AUTONOMOUS_INTERVAL_MINUTES=30
AUTONOMOUS_MAX_CYCLES=0  # 0 = infinite

# Data Management
BACKUP_RETENTION_DAYS=30
MAX_ARTICLES_PER_SESSION=20

# Development Settings
DEBUG=false
LOG_LEVEL=INFO
```

### Website Configuration
Edit `config/url_fetch.txt`:
```
https://thehackernews.com/
https://the420.in/
https://www.bleepingcomputer.com/
https://krebsonsecurity.com/
https://www.darkreading.com/
https://www.securityweek.com/
https://threatpost.com/
```

## ğŸ“Š API Documentation

### Base URL
- **Local Development**: `http://localhost:8080`
- **Docker Deployment**: `http://localhost:8080`
- **Documentation**: `http://localhost:8080/docs`

### Core Endpoints

#### News & Articles
```http
GET  /api/news              # Latest news with pagination
GET  /api/news/sources      # Available news sources
GET  /api/news/source/{id}  # News from specific source
GET  /api/news/search       # Search articles (q parameter)
GET  /api/article/{url}     # Get specific article details
```

#### System Information
```http
GET  /health                # Simple health check
GET  /api/health           # Detailed health information
GET  /api/stats            # System statistics
GET  /api/config           # System configuration
POST /api/config/reload    # Reload configuration
```

#### Alerts & Notifications
```http
GET  /api/alerts           # System alerts
GET  /api/alerts/stats     # Alert statistics
POST /api/alerts/mark-read # Mark alerts as read
POST /api/alerts/test      # Test alert system
```

#### Advanced Features
```http
POST /api/google-news/search    # Google News integration
GET  /api/google-news/trending  # Trending cybersecurity news
POST /api/extract              # Extract content from URL
POST /api/notify               # Send notifications
GET  /api/notifications        # Get notifications
```

### Example API Usage

#### Get Latest News
```bash
curl "http://localhost:8080/api/news?limit=5&page=1"
```

#### Search for Specific Topics
```bash
curl "http://localhost:8080/api/news/search?q=ransomware&limit=10"
```

#### Health Check
```bash
curl "http://localhost:8080/health"
# Response: {"status":"ok","timestamp":"2025-08-18T12:29:22.317218"}
```

## ğŸ³ Docker Deployment Guide

### Production Deployment
```powershell
# 1. Build and start containers
docker-compose -f docker\docker-compose.yml build --no-cache
docker-compose -f docker\docker-compose.yml up -d

# 2. Verify deployment
docker ps
curl http://localhost:8080/health

# 3. View logs
docker-compose -f docker\docker-compose.yml logs -f
```

### Container Management
```powershell
# Stop containers
docker-compose -f docker\docker-compose.yml down

# Restart containers
docker-compose -f docker\docker-compose.yml restart

# Update application
docker-compose -f docker\docker-compose.yml down
docker-compose -f docker\docker-compose.yml build --no-cache
docker-compose -f docker\docker-compose.yml up -d
```

### Volume Persistence
Data is automatically persisted in:
- `../data:/app/data` - News articles and processed data
- `../logs:/app/logs` - System logs and monitoring
- `../config:/app/config` - Configuration files

## ğŸ¤– AI Integration Details

### Google Gemini Configuration
1. **Get API Key**: Visit [Google AI Studio](https://aistudio.google.com/)
2. **Set Environment**: Add `GOOGLE_API_KEY=your_key` to `.env`
3. **Test Integration**: Run `python Ai\enhanced_ai_summarizer.py`

### AI Processing Features
- **Progressive Processing**: Handles 20 articles per session
- **Smart Summarization**: Optimizes content for readability
- **URL Tracking**: Prevents duplicate processing
- **Error Recovery**: Continues operation on individual failures
- **Quality Control**: Validates summaries before saving

### AI Output Format
```json
{
  "id": "article-123456",
  "original_title": "Original Article Title",
  "summary_title": "AI-Enhanced Title",
  "summary": "Intelligent summary of the article...",
  "source": "thehackernews.com",
  "url": "https://example.com/article",
  "timestamp": "2025-08-18T17:22:16Z",
  "processing_status": "success",
  "word_count_original": 1200,
  "word_count_summary": 150
}
```

## ğŸ“ˆ System Monitoring

### Health Monitoring
The system includes comprehensive health monitoring:

```json
{
  "status": "healthy",
  "timestamp": "2025-08-18T12:29:22.317218",
  "components": {
    "api_server": "operational",
    "data_files": "available",
    "ai_service": "connected",
    "backup_system": "active"
  },
  "metrics": {
    "total_articles": 47,
    "successful_sources": 6,
    "failed_sources": 1,
    "last_scrape": "2025-08-18T17:33:36.229",
    "last_ai_processing": "2025-08-18T17:42:30.123"
  }
}
```

### Log Management
Logs are automatically managed with rotation:
- `logs/fastapi.log` - API server operations
- `logs/backend.log` - System operations
- `logs/heartbeat.json` - Health monitoring data

### Performance Metrics
- **Scraping Speed**: ~45 articles in 20 minutes
- **AI Processing**: ~20 articles in 60 seconds
- **API Response**: < 100ms average
- **Memory Usage**: ~200-500MB total system

## ğŸ”§ Troubleshooting Guide

### Common Issues & Solutions

#### 1. Dependencies Installation Fails
```powershell
# Solution: Update pip and retry
python -m pip install --upgrade pip
pip install -r requirements.txt --force-reinstall
```

#### 2. API Server Won't Start
```powershell
# Check port availability
netstat -an | findstr :8080

# Kill process if needed
taskkill /f /pid <PID>

# Restart with different port
set UVICORN_PORT=9000
python main_launch.py --mode api
```

#### 3. Docker Build Fails
```powershell
# Clean Docker cache
docker system prune -f

# Rebuild without cache
docker-compose -f docker\docker-compose.yml build --no-cache
```

#### 4. AI Processing Fails
```powershell
# Check Google API key
echo %GOOGLE_API_KEY%

# Test AI directly
python Ai\enhanced_ai_summarizer.py

# Use environment file
copy env.example .env
# Edit .env with your API key
```

#### 5. No Data Files Generated
```powershell
# Check internet connection
ping google.com

# Test manual scraping
python main_launch.py --mode scrape --no-setup

# Verify URL configuration
type config\url_fetch.txt
```

### Debug Mode
Enable debug mode for detailed troubleshooting:
```powershell
# Set debug environment
set DEBUG=true
set LOG_LEVEL=DEBUG

# Run with verbose output
python main_launch.py --mode scrape
```

## ğŸš€ Advanced Usage

### Custom Automation Intervals
```powershell
# Every 15 minutes
set AUTONOMOUS_INTERVAL_MINUTES=15
python auto_start.py

# Every 2 hours
set AUTONOMOUS_INTERVAL_MINUTES=120
python auto_start.py
```

### Multiple Environment Setup
```powershell
# Development environment
python main_launch.py --mode setup
python main_launch.py --mode api

# Production environment (Docker)
docker-compose -f docker\docker-compose.yml up -d

# Testing environment
python main_launch.py --mode test
```

### Data Export & Integration
```python
# Python integration example
import json
import requests

# Load local data
with open('data/summarized_news_hf.json', 'r') as f:
    summaries = json.load(f)

# Or use API
response = requests.get('http://localhost:8080/api/news')
articles = response.json()['articles']

# Process data
for article in articles:
    print(f"Title: {article['title']}")
    print(f"Source: {article['source']['name']}")
    print(f"Summary: {article.get('summary', 'No summary available')}")
```

## ğŸ”’ Security Considerations

### API Security
- **CORS Configuration**: Properly configured for cross-origin requests
- **Rate Limiting**: Built-in protection against abuse
- **Input Validation**: All inputs validated and sanitized
- **Error Handling**: Secure error responses without information leakage

### Data Security
- **Local Storage**: All data stored locally by default
- **No External Dependencies**: Can run completely offline (except AI)
- **Secure Logging**: No sensitive data in logs
- **Environment Variables**: Secure configuration management

### Deployment Security
- **Non-Root Containers**: Docker containers run as unprivileged user
- **Volume Restrictions**: Limited container access to host system
- **Health Checks**: Automated monitoring and restart policies
- **Firewall**: Only necessary ports exposed

## ğŸ“‹ Maintenance & Updates

### Regular Maintenance Tasks
```powershell
# Weekly: Check system health
python main_launch.py --mode status

# Weekly: Clean old backup files (automatic, but verify)
Get-ChildItem data\backup -Recurse

# Monthly: Update dependencies
pip install -r requirements.txt --upgrade

# Monthly: Docker image updates
docker pull python:3.11-slim
docker-compose -f docker\docker-compose.yml build --no-cache
```

### System Updates
```powershell
# Update application code
git pull origin main  # If using git
# OR download latest version

# Update Docker deployment
docker-compose -f docker\docker-compose.yml down
docker-compose -f docker\docker-compose.yml build --no-cache
docker-compose -f docker\docker-compose.yml up -d

# Verify update
curl http://localhost:8080/api/stats
```

## ğŸ¯ Performance Optimization

### For High-Volume Usage
1. **Increase Workers**: Modify Docker `uvicorn.conf` for more workers
2. **Database Integration**: Add PostgreSQL/MongoDB for large datasets
3. **Redis Caching**: Implement caching layer for frequently accessed data
4. **Load Balancing**: Use nginx reverse proxy for multiple instances

### For Resource-Constrained Environments
1. **Reduce AI Batch Size**: Lower `MAX_ARTICLES_PER_SESSION` in config
2. **Longer Intervals**: Increase `AUTONOMOUS_INTERVAL_MINUTES`
3. **Disable Features**: Skip AI processing or use fewer sources
4. **Memory Optimization**: Use single worker in Docker deployment

## ğŸ¤ Contributing & Support

### Development Setup
```powershell
# 1. Fork the repository
# 2. Create development environment
python -m venv venv-dev
.\venv-dev\Scripts\Activate.ps1
pip install -r requirements.txt

# 3. Create feature branch
git checkout -b feature/your-feature-name

# 4. Test your changes
python main_launch.py --mode test
python -m pytest tests/  # If tests exist

# 5. Submit pull request
```

### Getting Help
1. **Check Logs**: Always check `logs/` directory first
2. **Health Check**: Run `python main_launch.py --mode status`
3. **Test Individual Components**: Use mode flags to isolate issues
4. **Documentation**: This README covers most scenarios
5. **Community**: Submit issues with detailed error messages

## ğŸ“Š Success Metrics

### System Performance Indicators
- âœ… **Scraping Success Rate**: 85%+ (6-7 out of 7 sources)
- âœ… **API Response Time**: < 100ms average
- âœ… **Data Freshness**: Updates every 30 minutes
- âœ… **AI Processing**: 20 articles per minute
- âœ… **System Uptime**: 99%+ with Docker deployment
- âœ… **Memory Usage**: < 500MB total system

### Data Quality Metrics
- âœ… **Article Deduplication**: 100% (no duplicate URLs)
- âœ… **Content Quality**: AI-enhanced summaries
- âœ… **Data Integrity**: Automatic backup and validation
- âœ… **Real-time Updates**: Live JSON feeds
- âœ… **Historical Data**: Automatic archival system

---

## ğŸ‰ Conclusion

This cybersecurity backend system provides a comprehensive, production-ready solution for:
- **Automated news aggregation** from multiple sources
- **AI-powered content enhancement** with Google Gemini
- **Scalable API infrastructure** with FastAPI
- **Enterprise-grade deployment** with Docker
- **Intelligent data management** with automated backups

**Ready to get started?** Run `python auto_start.py` and you'll have a fully operational cybersecurity intelligence system in minutes!
